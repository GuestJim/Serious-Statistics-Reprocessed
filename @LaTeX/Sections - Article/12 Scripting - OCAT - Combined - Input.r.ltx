\section{Scripting: OCAT -- Combined -- Input.r}

Some time ago I decided to split up the R script that handled the processing of the data to separate the code for reading in the data and that for generating the statistics and graphs.
The reason was so that if I was tweaking this output code, I would not need to make the changes to multiple files, but one that would then be referenced by the input code.
This is the current version of that Input.r script and as the idea is to have it control everything in the Output.r script, it has a number of controls in it, and my preference is to have such controls near the top.
This means the top of the file has a lot of variables being set to TRUE or FALSE, as we will shortly see, but there is some other code ahead of these switches, but this is not always the case as we will see in the Output.r script.

\subsubsection{R Library Loading}
\begin{styleR}
library(readr)
library(ggplot2)
library(moments)
\end{styleR}

Technically the Input.r script only actually needs the first library here to be loaded, the \textit{readr} library as the \textbf{read\_csv} function will be used later, but there is no issue with loading the other two in now.
I also have a preference to load the libraries I want at the top of the script.
The \textit{readr} library expands R's ability to import and export data to certain file types.

The \textit{ggplot2} library provides the functions and formatting for creating and saving the graphs I use.
There is a single line coming up soon that does use \textit{ggplot2}, but I could likely move that to the Output.r script without issue.
The \textit{moments} library is just needed for the skewness and kurtosis calculating functions, but I load it here because of my preference.

There is one exception to my preference of having the libraries loaded here and that is the \textit{tableHTML} library.
The HTML-related code is a relatively recent addition to the Output.r script, but very desirable as building HTML tables is not much fun.
When I was figuring out how to get that code to work as I wanted, I kept all of the relevant code in one location for easier troubleshooting, and this includes the loading of that library.
I could easily move it but I like the idea of keeping it all together, in part because of the special formatting I needed to put in place.

\subsubsection{Setting Title and Current GPU}
\begin{styleR}
game	=	"!GAME!"
cGPU	=	!GPU!

gameF	=	gsub(":", "-", game)
gameF	=	unlist(strsplit(gameF, split=" [(]"))[1]
\end{styleR}

The first two lines here are to set variables for the name of the game, or article title, and the current GPU.
The values for both are placed here by the Python script covered in the previous section.
Because the game name, or article title, will always be a string and should always be provided, the double quotes to identify it as a string are here.
They are missing for \textbf{cGPU} because for the multi-GPU situation it should be \textbf{NULL}, which is a special word in R and thus should not be in a string.
The Python script will add the quotes when they are necessary

The next two lines are for creating a file name-safe version of the game or article title.
This is done by using the \textbf{gsub} function, which expects the first argument to be the string pattern it is to replace, the second is the pattern to replace it with, and then the third is the string it is performing the operation on.
You can change this ordering by simply setting the names of the arguments equal to the values, but I find it easier to just use the expected order.
Here is an example with the names provided:

\subsubsection{Making Title File-name Safe}
\begin{styleR}
gsub(pattern = ":", replacement = "-", x = game)
\end{styleR}

The last line does something that is actually pretty similar to things I was doing in Python, which is to split a string at certain points, and then select a specific portion of it.
First the \textbf{strsplit} function is used to split the \textbf{gameF} variable at the opening of parentheses.
The output it provides is not a simple vector though, but an actual list and that is not what I want.
To fix that I use the \textbf{unlist} function that simplifies a list into a vector.
I then select the first portion of the vector, which will be the portion in front of the open parenthese, so if parentheses were used, everything they contain will be removed by this.

\subsubsection{General Graph Formating Control}
\begin{styleR}
theme_set(theme_grey(base_size = 16))
DPI			=	120
ggdevice	=	"png"
\end{styleR}

These three lines all have to do with certain controls for the graphs.
The first is setting the base font size to be used, and it is greater than the norm for \textit{ggplot2}.
Changing theme settings for \textit{ggplot2} can look a little weird, but fortunately it tends not to come up much.

Both the \textbf{DPI} and \textbf{ggdevice} variables are for controlling the saving of the graphs.
I think setting the DPI does not require explanation, but the \textbf{ggdevice} might.
It is possible to save graphs to many formats, such as PNG, what I usually use, and PDF.
You control this by setting the device the \textbf{ggsave} function uses, which we will see later.
If I want a PDF output, I would change this variable to be PDF, but while having a graphic version can be nice for fine details, they can also be as large as the data itself, making a PNG of limited resolution better.
I have also configured things so I can set a value of "both" and have both a PNG and PDF created

\subsubsection{General Graph Design Control}
\begin{styleR}
useSHORT	=	TRUE
testAPI		=	FALSE
listFPS		=	NULL

diffLim		=	NULL
QUAN		=	c(0.01, 0.99)
FtimeLimit	=	1000/15
yratesEXT	=	NULL

gWIDTH	=	8
gHEIGH	=	9
app.BREAK	=	FALSE
testQUA		=	FALSE
\end{styleR}

I have made numerous changes since I originally wrote this article, including adding functionality and shuffling some things around, to hopefully be easier to find.
This list of switches and controls I have just provided demonstrates both such changes as I have moved up some more useful controls and added new ones.

The top control here is \textbf{useSHORT} and controls if the shortened versions of Location and API names should be used.
Chances are if shortened versions are provided, they should be used and the function that actually applies them does check if they exist first.

The next is \textbf{testAPI} and this is to set if there are multiple APIs to be tested.
The default is FALSE as normally there is only one API to deal with.
If one forgets to change it to TRUE when there are multiple APIs in the data though, there is a check later that will set this to TRUE.
Really the purpose of the switch here is to override this later check on the data.
This way if the situation is one API in the current data sample but multiple APIs in the larger data population, that one API will still be identified in the various outputs.
The data for this article is an example of this scenario, as I could generate graphs for a specific configuration and if \textbf{testAPI} is FALSE, the RX Vega 64, DirectX 11, Max graphs will not indicate the API.

The \textbf{listFPS} variable allows one to add frame rates to that will have their corresponding percentiles provided using ECDF.
The variable can be set to a single value or a vector of multiple, but it should be noted the value will also act as an end point.
There is a default list of frame rates to do this for in the Output.r script (60, 50, 30, 20, 15) and when you add values to it with \textbf{listFPS} the output statistics will include every value from 60 to what you provide.
For example, if you set \textbf{listFPS} to 45, then the output will include the values for 60, 50 and 45, and if you set it to 90, the output will give the values for 90 and 60.
If you make \textbf{listFPS} the vector c(90, 45) then the output will include the ECDF values for 90, 60, 50, and 45, so the output will expand in both directions, including all default values contained within, and the ECDF of 60 FPS will always be included.

The next control is \textbf{diffLim} and concerns the Consecutive Difference graph, specifically its Y-scale.
Normally this scale stretches from -20 ms to 20 ms, but there are times the data goes outside this range, and when this happens \textbf{diffLim} can be given a value large enough to include all of the data.
Supplying a new \textbf{diffLim} value does not change the saved Consecutive Difference graph, but generates a new one with the extended scale.
This way there is a version of the graph that will have the same scale as every other Consecutive Difference graph for the data.

The \textbf{QUAN} variable is for controlling the quantiles used for the slope of the theoretical lines on the QQ plots, which is why it is c(0.01, 0.99); from 1\% to 99\%.
Though usually this range is appropriate for the data, sometimes there are enough long frame times that pulling in the 99\% limit is desirable, if only to satisfy curiosity.
It is also set up so whatever these limits are, they will be added to the breaks on the graph too, in addition to the normal 0.1\%, 1\%, 50\%, 99\%, and 99.9\% breaks.

The \textbf{FtimeLimit} variable is the frame time limit and serves as the upper limit for one scale on every graph.
As you can see, the default takes it to the equivalent of 15 FPS, but for this article it is at 16.667 ms, the 60 FPS equivalent.

Like the previous control, \textbf{yratesEXT} impacts the frame time scales by allowing additional breaks to be added.
The default list of breaks, set in the Output.r script, is 120, 60, 30, 20, 15, 12, and 10.
The values provided with this variable are added to that list, for those situations more breaks are desired.

The \textbf{gWIDTH} and \textbf{gHEIGHT} variables are for controlling the width and height of the graphs, which can be very necessary when a large number of plots are contained in a single graph.
These values can be changed here, to impact every graph saved, or at the time when the graphs are saved, in case one should be larger or smaller than the others.

The \textbf{app.BREAK} switch is a new addition and is to control applying line breaks to the graph labels.
Sometimes the graphs are not wide enough for all of the labels on the X-scale to fit without writing on top of each other, but by adding alternating line breaks to the labels, this can be avoided.
Sometimes this will be necessary, but not always which is why this switch exists.
Like \textbf{gWIDTH} and \textbf{gHEIGHT}, this could be changed just before a specific graph, though that would require editing the Output.r script.

\subsubsection{General Output Control}
\begin{styleR}
textOUT		=	TRUE
HTMLOUT		=	TRUE
graphs		=	TRUE
graphs_all	=	FALSE
\end{styleR}

These switches are to generally control what outputs are produced.
The \textbf{textOUT} switch controls all text output, including the HTML files, which can be separately turned off with \textbf{HTMLOUT}.
The \textbf{graphs} switch is like \textbf{textOUT} in that it disables all graphs while \textbf{graphs\_all} controls something else.
Later in this script the \textbf{INDIV} function is defined and its purpose is to allow for individual sub-samples to the Output.r script, such as separate APIs or Locations, and this switch controls if that happens.
The text outputs will also be produced when \textbf{graphs\_all} is enabled, as are the calls to \textbf{INDIV}.

\subsubsection{Data Type Output Control}
\begin{styleR}
textFRAM	=	TRUE
graphFRAM	=	TRUE

textDISP	=	FALSE
graphDISP	=	FALSE

textREND	=	FALSE
graphREND	=	FALSE

textDRIV	=	FALSE
graphDRIV	=	FALSE
\end{styleR}

There are multiple columns and thus data types present in the OCAT CSVs and I have the Output.r script able to handle four of them currently, with these switch pairs controlling which should be used.
Normally you want the FRAM pair enabled, as these are the frame time controls, but there can be times you want the display time, which is controlled by the DISP switches.
The REND and DRIV switches are for render time and estimated driver lag, respectively.
It is rare that these last two have a useful purpose, but they have come up and I have everything configured for them.

\subsubsection{API and Location Specific Text Output Switches}
\begin{styleR}
textAPI		=	FALSE
textLOC		=	FALSE
\end{styleR}

These two switches determine if the text outputs are produced for every API and Location present in the data and are normally left to FALSE.
These outputs are not really that necessary and are limited in their application as well.
Besides, the \textbf{INDIV} function given at the end is able to produce the same outputs along with graphs, when configured correctly.

\subsubsection{Consecutive Difference Data Creation Switches}
\begin{styleR}
textDiff	=	FALSE
graphDiff	=	FALSE
\end{styleR}

The \textbf{textDiff} and \textbf{graphDiff} variables are similar to the switches earlier for the different data types but involve an additional step.
When either of these are enabled, Consecutive Difference columns will be created and attached to the data for both MsBetweenPresents (frame time) and MsBetweenDisplayChange (display time), as it is not something the original data contains.
That being said, though, there is currently nothing configured in Output.r to work with these new columns so these should remain off until then.
Such configuration would not be hard to do, but I have not felt the need to do it yet.

\subsubsection{Setting Working Directory}
\begin{styleR}
if (interactive())	{
	setwd("!PATH!")
}	else	{
	pdf(NULL)
}
\end{styleR}

This code may look simple, but it is actually doing some fairly important things.
To understand this I need to explain the two ways I can and do run R code.
One is I just execute an R script and let it go in its own console window.
The other is to run the code in the GUI that is installed with R.
The next paragraphs cover the differences between these methods that are relevant to this code.
To distinguish between whether I am in the GUI or console, I use the \textbf{interactive} function, which is TRUE for the GUI and FALSE for the console.

When I am using the GUI it assumes the working directory is the location of its executable, which makes the \textbf{setwd} function, for setting the working directory, necessary.
The path is provided by the Python script, replacing !PATH!.
When running a script directly though, it will use its location as the working directory, which is perfect as that is the location I keep everything, or everything is relative to.
This is why it is necessary to set the working directory for the GUI, but not for the console.

While the direct script has the advantage with the working directory, it will also generate the rplots.pdf file, which I believe serves as a kind of intermediate file for graphs.
I do not want this file and a way to disable it is to use the \textbf{pdf(NULL)} command.
The GUI, however, does not produce this file, so the command is not necessary with it.

The purpose of this code block then is to address the disadvantages between the two methods of running R code, and I do not need to think about it when running the code.
Whichever method I am using, running this code handles the disadvantage and I do not need to think about it past that.

\subsubsection{Relative Path and Reading TXT Files}
\begin{styleR}
relPath	=	paste0(unlist(strsplit(getwd(), "OCAT Data"))[1], "OCAT Data")

txtFIND	=	function(TXT, rel.Path = relPath)	{
	locFILE	=	paste0(rel.Path, "/", TXT)
	if (file.exists(locFILE))	return(readLines(locFILE, warn = FALSE))
	return(NULL)
}
\end{styleR}

The \textbf{relPath} variable returns!
If you read the section for OCAT – Modular– PA.py then you will already know what it is, but for those who skipped to this section, I will quickly explain.
This variable stands for relative path and records where the OCAT Data folder is, because all of the data and relevant files are placed relative to that.
To find it, I have R get the current working directory with \textbf{getwd()} and then use the \textbf{strsplit} function for string split to split the path at "OCAT Data."
The output of this function is a proper list, which is not what I want to work with, so I use the \textbf{unlist} function to make it a vector.
The first element is then selected, as that will be the path to the OCAT Data folder, and then OCAT Data is concatenated onto the path, so we have the complete path to and including the folder.

Making \textbf{relPath} is all just for this next function to work, but it is a very useful function.
Something I was not aware of for some time is that R does have some file management capabilities, and by taking advantage of them I can build some useful features into my scripts, such as finding a text file and then reading its contents in.
That is the purpose of \textbf{txtFIND} that takes arguments of the name of the text file (including extension) and \textbf{rel.Path} for the path to look for the files.
As the files I will be looking for are usually in the OCAT Data folder, the \textbf{relPath} value is the default.

The body for the function starts by making the \textbf{locFILE} variable to store the location of the file.
It just pastes the name of the file onto the \textbf{rel.Path} value with "/" as that is what R uses to separate directory levels.

The next part of the body uses an \textbf{if} statement to check the file exists with \textbf{file.exists}.
If it does then I immediately use the \textbf{return} command to provide the output of \textbf{readLines} of the file.
The \textbf{readLines} function does what you would guess; it reads in the lines of a file and produces a vector.
It does need to have its argument \textbf{warn} set to false because otherwise it throws a warning about an incomplete final line.
Warnings are just warnings though, not errors, so there is no problem here and that is why I want the warnings disabled.

When reaching a \textbf{return} command in a function, R will return whatever it is meant to and then end the function, so if the file is found, it returns the lines it read and finishes.
If it does not find the file, then the second \textbf{return} will be executed, which returns NULL, a special word in R that basically translates to an empty value and will not break many functions and operations.

\subsubsection{GPU List}
\begin{styleR}
listGPU		=	c(
"RX 580",
"RX Vega 64",
"GTX 770",
"GTX 980",
"GTX 1070",
"GTX 1080",
"RTX 2060",
"RTX 2080"
)
\end{styleR}

Though it might not seem a best practice to keep an explicit list of the GPUs in the file, there are some advantages.
The most important to me is that the order is also set, so the AMD and NVIDIA GPUs are separated and sorted in terms of generation and relative performance.
The other advantage I do not take advantage of, but might at some point, is the ability to assign colors for each GPU.
Currently I allow the colors to be automatically assigned, but if I do decide to manually assign colors, it will be easiest to do with an explicit list.

Of course the disadvantage to having an explicit list is that when a GPU is added to my collection, I will need to update the list.
Of course updating it in this reference file will see it propagate out, but it will still be a necessary step.
Removing a GPU from the collection without updating the list will not be an issue, as this list determines order, but something like the automatically assigned colors is based on the values in the data.
Indeed this article demonstrates this as the complete GPU list is in the files but I only used two without any issue.

\subsubsection{Reading Quality, Location, and API Lists}
\begin{styleR}
listQUA		=	txtFIND("Qualities.txt")

listLOC		=	txtFIND("Locations.txt")
shortLOC	=	txtFIND("Locations Short.txt")
levsLOC		=	listLOC
if	(useSHORT	&	!is.null(shortLOC))	levsLOC	=	shortLOC

listAPI		=	txtFIND("APIs.txt")
shortAPI	=	txtFIND("APIs Short.txt")
levsAPI		=	listAPI
if	(useSHORT	&	!is.null(shortAPI))	levsAPI	=	shortAPI
\end{styleR}

Now we finally get to see the \textbf{txtFIND} function put to use as it looks for certain text files and assigns the resulting vector to variables.
At the top we have the list of Qualities read in and assigned to \textbf{listQUA}.
There is a bit more that needs to be done with this vector though, as there is nothing here to cover the possibility of the Qualities.txt file being missing, which is reasonable for reviews.
I have something to take care of that later, but it is necessary for the data to be loaded in first, and right now I want all of these lists created first.

The next two blocks are similar in structure, so I am just going to cover the top one, concerning locations.
First \textbf{txtFIND} is used on Locations.txt, and if it is found then its contents are assigned to \textbf{listLOC} as a vector.
If there is no file, then the variable will be NULL and I handle that later.

Next \textbf{txtFIND} is used to assign the contents of "Locations Short.txt" to \textbf{shortLOC}, so if a list of shortened location names is present, that is now loaded in.
To help generalize the functions for applying shortened names, the \textbf{levsLOC} variable is made to store the location levels.
The purpose is to make the Locations column in the data factors, a special data class in R that maps the values to levels.
For this data, the values would be Hatsheput, Dunes, and Thebes – Karnak but when made into factors, they will become 1, 2, and 3 with 1 mapping to Hatsheput, 2 to Dunes, and you get the idea.
The names are the factor levels, hence the name \textbf{levsLOC} and \textbf{levsAPI} later.

By default, \textbf{levsLOC} should be \textbf{listLOC} because unless there are shortened names, \textbf{listLOC} holds the desired level names.
The next line handles if \textbf{shortLOC} exists with the \textbf{is.null} function, because if there is no "Locations Short.txt" file, \textbf{shortLOC} will be NULL.
That is not the only condition being used with this \textbf{if} statement though as there is also the switch \textbf{useSHORT} to check, so all shortened names can be disabled.
If we do want to use shortened names, which will usually be the case, and if they were provided, then \textbf{levsLOC} will become \textbf{shortLOC} assigned to it.

\subsubsection{Importing Combined Data}
\begin{styleR}
if (file.exists("@Combined - !QUA!.csv.bz2"))	{
	resultsFull	=	read_csv("@Combined - !QUA!.csv.bz2")
}	else	{
	resultsFull	=	read_csv("@Combined - !QUA!.csv")
}
\end{styleR}

With the lists for factor levels set, I now have the data loaded in, but the process has to be a little more complicated as it might be compressed.
This complication is not much though as it just checks if the compressed file exists, and if it does that file is imported, but if it does not exist, then the uncompressed CSV is imported instead.
This does assume that file exists, but it should be a safe assumption.

\subsubsection{Ordered Factor Level Application}
\begin{styleR}
resultsFull$GPU		=	ordered(resultsFull$GPU,		levels = listGPU)
resultsFull$Quality	=	ordered(resultsFull$Quality)
if	(!is.null(listQUA))	resultsFull$Quality		=	ordered(resultsFull$Quality,	levels = listQUA)

resultsFull$Location	=	ordered(resultsFull$Location)
if	(!is.null(listLOC)) resultsFull$Location	=	ordered(resultsFull$Location,	levels = listLOC)

resultsFull$API		=	ordered(resultsFull$API)
if	(!is.null(listAPI)) resultsFull$API			=	ordered(resultsFull$API,		levels = listAPI)
\end{styleR}

With the data having been loaded into \textbf{resultsFull} it is time to apply some formatting to it, thanks to the lists put together above.
By using the \textbf{ordered} function on a column, it will make the content of that columns factor and order them according to the levels list I provide.
If ordering was unimportant, the \textbf{factor} function could be used and with the same arguments even.
Indeed the difference between \textbf{factor} and \textbf{ordered} is the \textbf{ordered} argument that has a default of FALSE for \textbf{factor} and is implicitly TRUE for \textbf{ordered}.

With that first line, the GPU column is converted to ordered factors within the \textbf{ordered} function and then applied to the column in \textbf{resultsFull}.

The subsequent lines all share a similar design, though the importance of each varies.
The design is to first make the column ordered factors, which will be done with the levels being based on whatever the contents of the columns.
Next a check is made for if a list of levels is not NULL, and if that is TRUE, then the list of levels is applied, potentially changing the order of the factors.
This does not disrupt the data in the columns but this design is necessary for covering those situations when the lists are not present.

Most commonly, there will not be any APIs involved in the testing, which will result in the column being all empty values and not a problem.
The next most likely to happen is I neglect to provide a list of qualities, but because there is a folder name it will be present in the data regardless.
This means \textbf{listQUA} will be NULL and not applied as the levels, but the column will still be made ordered factors.
Of course, this is most likely to happen with a review when I test with only one quality, so the lack of manually ordered factors is hardly an issue.
It is very unlikely I will ever forget to record location names, if only because I typically take some time to think about the best location to collect data or decide while I am playing, and so write it down to remember to come back.

\subsubsection{Protecting Data from Manipulation}
\begin{styleR}
lockBinding("resultsFull", .GlobalEnv)

results	=	resultsFull
\end{styleR}

These lines both exist for a similar purpose and that is to protect the now formatted, original data from being altered by any operation or function.
First we have the \textbf{lockBinding} function, which I only learned of by accident but will apparently allow one to make an object read-only, as it were.
To do this the name of the object, so variable name, is passed to it as a string, and the environment it should be locked in is also specified.
The global environment, identified here as \textbf{.GlobalEnv}, is the environment one normally works in and that other environments, like those made by functions will inherit things from.

The second line further protects \textbf{resultsFull} by assigning its contents to \textbf{results}, so I can work with and manipulate that object instead, providing another layer of proection to the original data.

These layers of protection might not be necessary anymore though, as everything that may alter \textbf{results} is now done within functions and their environments.
This means the alterations are lost upon exiting the function, but there might still be times when troubleshooting that the data is altered, so this protection layer is not undesirable.

\subsubsection{Multi-GPU and Test-API Checks}
\begin{styleR}
multiGPU	=	is.null(cGPU)
labsGPU		=	labs(caption = cGPU)
if	(multiGPU)	labsGPU	=	labs()
if	(!testAPI)	testAPI		=	(length(unique(results$API)) >= 2)
\end{styleR}

This code is concerned with intelligently checking things to properly format the statistics and graphs later on.
First \textbf{multiGPU} is set to be TRUE or FALSE based on if the \textbf{cGPU} variable is NULL or not.
It is NULL when there are multiple GPUs, and thus no current GPU, so the \textbf{is.null} function will return TRUE.

The next two lines are for controlling if the graphs have the caption label present, and what it is.
The first line sets \textbf{labsGPU} such that the graphs will have a caption on them identifying the current GPU.
If this is a multiGPU scenario, however, then the \textbf{labs} layer is made blank, removing the caption.

The final line is a check on \textbf{testAPI}, because one might forget to set it TRUE when there are multiple APIs.
If that happens, then \textbf{testAPI} is false so the \textbf{if} statement passes and a new value is given to \textbf{testAPI} based on the number of APIs in the data.
By using \textbf{unique} we get a list of the unique APIs in the data and then the \textbf{length} function returns how many are in that list.
The result of the check of it being greater than or equal to 2 is then stored assigned to \textbf{testAPI}, so if there are two or more APIs, it will be made TRUE.

\subsubsection{Data Groupings Construction}
\begin{styleR}
GROUPS	=	list(GPU = results$GPU, API = results$API, Quality = results$Quality, Location = results$Location)
if	(!testAPI)	GROUPS$API		=	NULL
if	(!testQUA)	GROUPS$Quality	=	NULL
\end{styleR}

Now here is a trick I wish I was aware of long ago.
The \textbf{GROUPS} variable is an actual list in R, not just a vector, and I use it to identify the various groups within the data.
These would be the GPU, API, Quality level, and Location, and in this list those names and their respective columns are paired.
The trick I was not aware of previously was how easily an element in a list can be removed.
You see, there are plenty of times I do not want to have \textbf{GROUPS} include certain things, like the API and Quality, because they are the same for all of the data.
How I handled this before was to create a new version of \textbf{GROUPS}, but now I know that just setting an element in a list to NULL removes it from that list.

The way it works here is first \textbf{GROUPS} is made with every group the data may have.
After this, the \textbf{testAPI} and \textbf{testQUA} variables are checked to determine if either API or Quality is being tested within the data.
If it is not being checked, then that element in \textbf{GROUPS} is set to NULL, removing it.

If you have previously looked at these scripts, you might wonder why \textbf{GROUPS} is even in Input.r, as previously it was only present in Output.r for use with the \textbf{aggregate} function.
The reason is in the next section of code.

\subsubsection{Custom Consecutive Difference Function and Special Column Creation}
\begin{styleR}
diff.CONS	=	function(DATA, DIR = "Forward", lag = 1)	{
	if	(DIR == "Forward")	return(c(diff(DATA, lag = lag), rep(0, lag)))
	if	(DIR == "Backward")	return(c(rep(0, lag), diff(DATA, lag = lag)))
}

if (textDiff	|	graphDiff)	{
	results$MsDifferencePresents		=	unlist(by(results$MsBetweenPresents, GROUPS, diff.CONS))
	results$MsDifferenceDisplayChange	=	unlist(by(results$MsBetweenDisplayChange, GROUPS, diff.CONS))
}
\end{styleR}

This is the code concerned with adding the Consecutive Difference columns to the data, even though these columns have no use currently.
First is a custom function for producing the consecutive differences and then how it is applied to \textbf{results}.
It would be inappropriate to directly apply any version of the \textbf{diff} function to the data because of the grouping, but R's \textbf{by} function takes care of this.
This function will use \textbf{GROUPS} to identify the groupings within the data and then apply the provided function to these groups individually, which is a far easier and faster method than the nested \textbf{for} loops I used previously.

Getting back to the code, the \textbf{diff.CONS} function combines the \textbf{diff} function with a zero placed at the beginning or end of the output, depending on the direction the differences should point to.
I prefer for the direction to be forward, which means the sum of a recording in \textbf{DATA} and the difference value in the same row in the data equals the recording in the next row.
This is why "Forward" is the default value of \textbf{DIR}, the direction argument.

The third argument will not be used in these scripts but is necessary for a different script, so for consistency, it is here.
The \textbf{lag} is an argument for the base \textbf{diff} function, controlling if it is consecutive values the difference is found between or not.
To explain, suppose a list c(1, 2, 3, 4).
With a \textbf{lag} of 1, the default, the \textbf{diff} will return c(1, 1, 1), but with a \textbf{lag} of 2 it will return c(2, 2), because the differences between 1 and 3, and 2 and 4 are both 2.
As I said, this is not necessary in these scripts, but I have others where it is necessary, so it is here for consistency.

Entering the body of the function, a check is made on the value of \textbf{DIR} and immediately the \textbf{return} command is given with the appropriate operations provided, because the work here really is simple.
it is just helpful to have it in its own function.
Going inside of the operations, \textbf{diff} is applied to \textbf{DATA} and the \textbf{lag} value is supplied as well.
The output of this is combined with zeroes then, at the beginning or end based on the \textbf{DIR} value, and the number of zeroes based on the \textbf{lag} value.
As that example earlier showed, the output from \textbf{diff} will be shorter than the input, which is a problem for the graphs that require the data be rectangular (all columns having the same number of rows and all rows the same number of columns).
This is why zero padding is applied, and as the example also showed, the amount of padding is dependent on the \textbf{lag} because the value pairs to find the difference of run out sooner.

With \textbf{diff.CONS} created, we now get to the code that will create the MsDifferencePresents and MsDifferenceDisplayChange columns, if either \textbf{textDiff} or \textbf{graphDiff} is TRUE.
If they are, then the \textbf{by} function is given the appropriate column of the data frame and the \textbf{GROUPS} list made earlier, so it knows what the groupings within the data should be, and lastly the function, \textbf{diff.CONS} is provided, so it knows what to run.
If I wanted to supply an argument for \textbf{diff.CONS}, I would pass it to \textbf{by} like any other argument, and it will be passed through accordingly.

While \textbf{by} is definitely a useful function for a use like this, it is also unusual in that its output is of class "by" and I have no idea what that data type is.
I do know it cannot be applied to a data frame, which is why the \textbf{unlist} function is here as it solves that problem.

\subsubsection{Descriptive Name Function}
\begin{styleR}
DESC	=	function(ITEM = NULL)	{
	descs	=	list(GPU = unique(as.character(results$GPU)), API = unique(as.character(results$API)), Location = unique(as.character(results$Location)), Quality = unique(as.character(results$Quality)))
	
	if	(length(descs$GPU)		> 1)	descs$GPU		=	NULL
	if	(length(descs$API)		> 1)	descs$API		=	NULL
	if	(length(descs$Location)	> 1)	descs$Location	=	NULL
	if	(length(descs$Quality)	> 1)	descs$Quality	=	NULL
	
	gameQ	=	game
	if	(!is.null(descs$Quality))	gameQ	=	paste0(game,	" - ",	descs$Quality,	" Quality")
	
	gameGAQF	=	paste0(gameF,	" - ",	paste0(descs,	collapse = " - ")	)
	gameGAQ		=	paste0(game,	" - ",	paste0(descs,	collapse = " - ")	)
	if	(!is.null(descs$Quality))	gameGAQ	=	paste0(gameGAQ, " Quality")
	
	if	(!is.null(ITEM))	{
		gameGAQF	=	paste0(gameGAQF,	" - ",	ITEM)
		gameGAQ		=	paste0(gameGAQ,		" - ",	ITEM)
	}
	return(c(gameGAQF,	gameGAQ, gameQ))
}
gameGAQF	=	DESC()[1]	;	gameGAQ		=	DESC()[2]	;	gameQ	=	DESC()[3]
\end{styleR}

Another of the newer changes I have made is to pull the process for generating descriptive file names and titles into a function; this \textbf{DESC} function.
Besides taking the time to improve how these names are generated, the \textbf{DESC} function will also work better with the \textbf{INDIV} function that is coming up next.
In fact, the interaction between them is why the \textbf{ITEM} argument exists, so the item the data is currently being subset by will be included in the appropriate names.

The first line of the function's body is to create a list of each descriptor type I may want in a name, and I have it set to use information from \textbf{results}, so it will be accurate.
I use the \textbf{unique} function because I do not care how often a specific descriptor occurs in the data, just that it does.
However, I cannot use this function directly on the desired columns because they are all factors.
Doing so would just return the numbers that are mapped to the factor levels, so to fix this I convert the factors to strings with \textbf{as.character}.
This conversion process triggers the mapping, and when combined with \textbf{unique}, just the levels present in those columns of \textbf{results} will be placed in the list.

The next step in \textbf{DESC} is to remove from the list any descriptor type that has multiple unique values in \textbf{results}.
Multiple values means within the data there will be a change, and that will make the names and titles I am trying to produce incorrect.
The solution is easy enough as \textbf{if} statements check if the length of each descriptor type in the \textbf{descs} list is greater than one, and if it is, that type is removed.
With \textbf{descs} now containing only those strings that describe all of the data in \textbf{results}, the strings for the titles and names can start being built.
First is \textbf{gameQ}, which will be the game name with the Quality, so it starts by having \textbf{game} assigned to it.
A check is then made for if Quality exists in \textbf{descs}, and if it does, then it will be appended to \textbf{gameQ} along with a hyphen to separate it and "Quality" after to identify it.
The variable is used as the title for the graphs.

The other two strings to be built are \textbf{gameGAQF} and \textbf{gameGAQ}, which are more descriptive titles but with different purposes.
Both will hold the game name, GPU, API, and Quality, but \textbf{gameGAQF} is meant for file names and so uses the \textbf{gameF} version of the game name and does not have "Quality" attached to it.
A check is done so \textbf{gameGAQ} only has "Quality" concatenated on the end if the Quality descriptor is present.

You can see I am using an argument for \textbf{paste0} I have not used before called \textbf{collapse}.
Normally \textbf{paste} and \textbf{paste0} when given a list or vector will concatenate to each element in the list or vector, but with the \textbf{collapse} argument not being NULL, they will instead collapse the object to a single string.
The value of \textbf{collapse} will then be used to separate the elements in the object, and I use a hyphen for this, making for a very clean method of getting the string format I want.

The next bit before returning the strings being built is a check for if an \textbf{ITEM} has been provided.
If the argument is not NULL, both \textbf{gameGAQF} and \textbf{gameGAQ} will have its value concatenated on the end.


With the strings made within the function, the \textbf{return} function is based a vector containing the strings.
As you can see after closing \textbf{DESC}, I then call it three times to apply these strings to copies of the variable that will be accessible outside the function.
It would be possible to have \textbf{DESC} assign these values to the global environment and make \textbf{return} unnecessary, but I would prefer to only do such super-assignment when necessary.

\subsubsection{Individual Subset Function}
\begin{styleR}
INDIV	=	function(COL, SUBS, useSHORT = useSHORT, gWIDTH = gWIDTH, gHEIGH = gHEIGH)	{
	if	(COL != "GPU")	dir.create(paste0("@", COL))

	for	(ITEM in SUBS)	{
		# COL	<<-	COL
		# SUBS	<<-	SUBS
		# ITEM	<<-	ITEM
		#	helpful for troubleshooting
		
		
		message(paste0("\n", ITEM))
		results	=	resultsFull[resultsFull[, COL] == ITEM, ]
		if (nrow(results)	==	0)	next
		if (COL == "GPU" & length(unique(results$API)) == 1)	next

		if	(COL != "GPU")	{
			FOLD	=	paste0("@", COL, "/", ITEM)
			dir.create(FOLD)
		}

		gameGAQF	=	DESC(ITEM)[1]	;	gameGAQ		=	DESC(ITEM)[2]	;	gameQ	=	DESC(ITEM)[3]
		if (COL == "GPU")	{
			gameGAQF	=	paste0(ITEM, "/", gameGAQF)
		}	else	{
			gameGAQF	=	paste0(FOLD, "/", gameGAQF)
		}

		perGPU	=	FALSE
		source("@Combined - Output.r",	local = TRUE)
	}
}
\end{styleR}

Here is the \textbf{INDIV} function that is actually fairly important and powerful, even though I likely will not use it often.
Its purpose is to take \textbf{resultsFull} and assign a subset of it to \textbf{results} before executing the Output.r script.
I have previously had code for doing this, but not formalized into a function, and making this function definitely confirmed a statement I made earlier about custom functions; making them drives me to produce better code.

First, this function has a collection of arguments, but except for two they all have defaults.
The two that lack defaults are for identifying the column to filter on and the list of subsets to filter by.
An example would be the Location column and then \textbf{listLOC}, so the data passed to Output.r will be for the individual locations, potentially across all GPUs.
Not exactly something I would expect to come up often, but that is not really the point.
It could come up and I wanted to make sure it would be possible.

The other arguments all use the same names as switches and controls set at the beginning of the file.
Perhaps I want the graph dimensions to be different or maybe the shortened names do not need to be used.
Normally the environment made by \textbf{INDIV} would inherit the values set earlier, so these arguments allow me to change them within its environment, without altering the original values.

Entering the body of the function, the first line checks if we are sub-setting by GPU, and if we are not \textbf{dir.create} is used to create a directory.
This directory will have an "@" symbol at the beginning, placing at the top of a sorted list, and the name of the column.
There are already folders for each GPU, so it is hardly necessary to create them.

Next the all-important \textbf{for} loop is created that will work through the \textbf{SUBS} list.
The first things within the loop are some commented out super-assignments.
As my comment on this code states, these are helpful for troubleshooting as the \textbf{COL}, \textbf{SUBS}, and \textbf{ITEM} iterator from the loop will become accessible outside of the function.
After this, we get to the actual work.

The \textbf{message} function is handy as it displays a message in the R window, whether it is a console window or the GUI, and here it will display the current \textbf{ITEM} value, so we know where in the loop it is working.
A line break is placed before this, so it will be separated from any other text in the window.

After the message the subset of \textbf{resultsFull} is created and assigned to \textbf{results}.
To do this, I am using the square bracket notation's ability to filter that will check if the value in the specified column of \textbf{resultsFull} is equal to the current \textbf{ITEM} value.
Only those rows where this condition returns TRUE will be passed on, and all of their columns will come too as nothing has been specified about them.

To keep the function from wasting any time or potentially throwing errors, the next line checks if this new \textbf{results} is empty or not.
If it is empty, the \textbf{next} command will be executed, which ends this iteration in the \textbf{for} loop and goes to the next.

Another check follows this last one, for if we are sub-setting by GPU and if there is only one API in the data.
If this is the case, then the graphs that would be generated would be no different from those made specific configurations.
This \textbf{INDIV} function is only useful when working with the larger population of data I collect, which is normally at a single Quality level, so filtering the data down to where it just mirrors the GPU, API, and Quality configurations I already look at is a waste of time.

The next block of code will create folders inside the one \textbf{INDIV} already made, for holding the results for the specific \textbf{ITEM} value, but first it checks we are not working on the GPU column.
If that condition is TRUE, then the \textbf{FOLD} variable is made to hold the folder name, and \textbf{dir.create} is called to make the folder.
The \textbf{FOLD} value is used later, again because of the possibility of sub-setting by GPU.

After the folder is created, the titles and names are made anew with calls to \textbf{DESC}.
Because it arrives at these names from the data in \textbf{results}, it will produce new names here that will more specifically identify the configuration.

We have one more \textbf{if} statement here, again checking if we are sub-setting by GPU or not.
If we are, then the \textbf{gameGAQF} variable will have the current GPU name appended to its beginning along with the "/" symbol, so the outputs will be saved in the appropriate folder.
If \textbf{INDIV} is not working on the GPU column, however, then the value of \textbf{FOLD} is attached to the front instead, again so the outputs will be saved in the appropriate folder.

The last two lines of \textbf{INDIV} both require a little explanation.
First is the \textbf{perGPU} switch, which is a new control I have added.
Its purpose is to disable the creating of text outputs for each GPU, a feature of the Output.r script for quite some time.
It can run into issues when filtering the data and having outputs put into different folders, so the feature is simply disabled.

The next line uses the \textbf{source} function to read in the Output.r script and execute it, which we will see later as well, but it is the second arugment that is most important.
Normally \textbf{source} executes the commands it imports within the global environment, and so not the environment \textbf{INDIV} has created with different values for \textbf{results} and more.
The \textbf{local} argument corrects this when TRUE, by executing the commands within the current local environment.

\subsubsection{Per-GPU Text Output Control and Calling Output.r}
\begin{styleR}
perGPU	=	TRUE	#	used for creating stats files in each GPU in their folders
if (!multiGPU)	perGPU	=	FALSE
source("@Combined - Output.r")

if	(graphs_all)	{
# INDIV("GPU",		listGPU,	useSHORT = TRUE,	gWIDTH = gWIDTH * 1.25,	gHEIGH = gHEIGH * 2)
# INDIV("Location",	listLOC,	useSHORT = FALSE,	gWIDTH = gWIDTH * 1.25,	gHEIGH = gHEIGH * 1)
# INDIV("API",		listAPI,	useSHORT = TRUE,	gWIDTH = gWIDTH * 1.25,	gHEIGH = gHEIGH * 1)
}
\end{styleR}

At last we are at the end of this script.
The first two lines of this final code block are concerned with the \textbf{perGPU} switch I described in the \textbf{INDIV} function.
It first has a manual value set of TRUE, but it will be changed if \textbf{multiGPU} is FALSE, indicating there is only one GPU in the data.

After that, the \textbf{source} function is used to execute the commands in the Output.r script, and except for the possible calling of \textbf{INDIV}, this script is done.

The final part of the script first checks if \textbf{graphs\_all} is TRUE, which it normally is not, and then has three examples of \textbf{INDIV} calls.
Considering I doubt I will use \textbf{INDIV} much, I have these examples here so I can remember how to use it, if it ever comes up.
One thing I am doing with these calls that I think is important is I am using relative changes to the graph width and height variables.
Explicit values can be provided if you wish, but this way the graphs can be grown or shrunk relative to the values set earlier in the script.

Now that this has finished, we can get to the Output.r script where all of the statistics and processed and the graphs created.
