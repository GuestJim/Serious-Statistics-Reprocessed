\section{Scripting: The Tools and Situations}
At last we are into the scripting half of this article, and because of how I tend to go in and edit my scripts when I have a new idea, it is hard to say how long these sections will remain accurate.
There are certain things I strongly doubt will change though, and that is the software tools I use, certain logistical structures, and the fact I collect data on five minute (300 second) runs.
Before getting too far though, I would like to remind everyone that I taught myself practically everything about to be covered, so for those of you who actually know how to write scripts in the languages I use, I am so sorry for what you are going to see.

Starting from the beginning, I use \href{https://github.com/GPUOpen-Tools/OCAT}{OCAT} to collect the data.
This is the Open Capture and Analytics Tool released as part of AMD's GPUOpen platform.
The simplest way to think of it is as a convenient front-end to PresentMon, but there is more to it than that now, and one of my personally favorite changes in recent versions was a tone playing at the start and stop of each run.
While I still will use a timer to help me keep consistent timing during the course of a run, hearing that notification the data collection has stopped is very satisfying.

I have OCAT set to collect for 300 seconds, as I mentioned above, but something I feel I should mention about my methodology is that I do not repeat my runs under the same configuration, without special reason to do so.
This is actually part of the reason I use 300-second runs, as the additional data should improve the confidence in the results, compared to shorter runs that might be just one minute.
Originally when reviewing a game I would have OCAT open in the background and when I thought I was in a good section to collect data for, I would hit the button and let it go while I continue playing.
A 'good section' is one where I expect to not trigger a cutscene or need to enter a menu, as either occurrence can significantly impact performance.
In recent months this is not how I have collected performance data, as I attempt to capture video of the review playthrough, and the load of video capture and encoding can, has, and will impact performance.

Once the data collection is finished, OCAT will close a CSV with all of the data, and append a line onto a summary file with various statistics.
I take the CSVs and place them into the appropriate folders, and this folder structure is important as some of the scripts depend on it.
The top folder is the name of the article, which in this case is "Serious Statistics and Scripts."
The next folder that matters here is "OCAT Data," and within it are folders for each GPU.
Under the GPU folder will be the API folder, and under that the graphics configuration such as High or Max, and finally we have the CSVs.
For this article only the GPU and API changes, so the folder structure looks like this: "Serious Statistics and Scripts/OCAT Data/*GPU*/*API*/Max/*.csv."
For a performance analysis, like that for \href{http://www.overclockersclub.com/reviews/tomb_raider_shadow_performance/}{\textit{Shadow of the Tomb Raider}}, the GPUs and graphics folder will be variable too: "Shadow of the Tomb Raider Performance Analysis/OCAT Data/*GPU*/*API*/*Quality*/*.csv."
For \href{http://www.overclockersclub.com/reviews/darksiders_3_performance/}{\textit{Darksiders III}} there was a significant difference, and one that took me a little bit to solve for in the scripts: "Darksiders III Performance Analysis/OCAT Data/*GPU*/*Quality*/*csv."
There is no API folder for this and most other performance analyses, because there is only the one API supported, and so the scripts that deal with this folder structure needed to be designed to work whether that folder is there or not.

Once the CSVs are in their folders, then I will drag and drop a file in the OCAT Data folder onto a Python script I wrote to generate all of the scripts I need.
Before covering that script though, I want to go through one of the reference scripts it will be working with, but before covering any scripts there are some things I want to address.
Specifically that there will be certain other files in the OCAT Data folder, and the other software tools involved.

I just mentioned \href{https://www.python.org/}{Python}, and to be more specific I am using version 3.7.0 and import the \textit{sys}, \textit{os}, and \textit{shutil} modules.
By now this version might be a little old, but I am confident my scripts will work until Python receives a major update.


Though Python does have data processing and graph generation capabilities, I use \href{https://www.r-project.org/}{R}.
It is an open source environment for statistical computing and graphics, so it will likely have better support for the data processing I am doing than Python.
It too can load in libraries to add features, and I use \textit{readr}, \textit{ggplot2}, \textit{moments}, and \textit{tableHTML}.
The \textit{moments} library is just for the Skewness and Kurtosis functions, and similarly \textit{tableHTML} is only for saving HTML tables with the desired formatting so I can pass the output directly into the articles.

The other files I will have in the OCAT Data folder are there because I wanted to make my life easier.
Previously I would need to manually edit the R scripts to have the graphs/plots properly labelled with graphics configuration, recording location, etc.
What I have since done is configured the Python script to check for and then read in files that I place this information in.
I commonly have a Locations.txt file but can also have Qualities and APIs files, and even Locations Short and Short APIs Short files if necessary.
(The Short files have shortened versions of the names, such as changing Thebes â€“ Karnak to just Karnak, so it fits better on the plots.These text files are all optional but very desirable.

In total I have four scripts (now) for managing and processing the data and one more for managing the graphs, and while the first I use in my process is the Python script, I am instead going to start with an R script.
The reason is that the Python script works heavily with this particular R script, and I feel this script will serve better as an introduction, though it does have a quirk or two.
The purpose of this script is to read in the multiple CSVs, combine them to a single table, and then save a new file with all of the relevant data.
While this can significantly increase the storage footprint of the data, it makes things a bit easier to have all of the data in a single file, instead of needing to load in multiple files to work with.

I am also somewhat inclined to explain that these scripts are going to have some potentially curious switches and controls in them.
Some of these are for configurability, such as turning on or off the use of display time, but some are to cover the different scenarios I run into.
In fact, I only recently implemented some of these scenario-sensitive switches, as I decided to unify the similar by separate scripts for the different scenarios (such as all of the High Quality data versus the data for a specific configuration) and abstracting some scripts, so they can function regardless of the scenario.

Something useful with R is that if you ever come across a function you want more information about, you can get it by entering a question mark and then giving the name of the function.
If the function is from a library, you will need to have loaded the library, and naturally the custom functions I have written will not be supported like this.

